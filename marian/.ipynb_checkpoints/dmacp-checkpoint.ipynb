{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSRdN7CxW4Ab"
   },
   "source": [
    "# DMACP data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Z_TKb7xGiEVz"
   },
   "outputs": [],
   "source": [
    "data_path = \"../dmacp_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdBCVePQW1IY"
   },
   "source": [
    "## Merge content files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6ypDamFizXw",
    "outputId": "31fbb586-8fcc-4e58-fd25-9e3fe759bf4e"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dmacp_data/contribution/uncalculated-risk.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5bf204a6871e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dmacp_data/%s/%s.json'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'slug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dmacp_data/contribution/uncalculated-risk.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "content_list_filepath = data_path+'/article_list.json'\n",
    "\n",
    "projects_data_filepath = data_path+'/article_list.json'\n",
    "\n",
    "merged_content_data = {\n",
    "    'contribution': {\n",
    "        'filepath': data_path+'/contributions.json',\n",
    "        'data': {}\n",
    "    },\n",
    "    'field_note': {\n",
    "        'filepath': data_path+'/field_notes.json',\n",
    "        'data': {}\n",
    "    },\n",
    "    'project': {\n",
    "        'filepath': data_path+'/projects.json',\n",
    "        'data': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def get_text_from_html(raw_html):\n",
    "    \n",
    "    # from https://stackoverflow.com/a/12982689\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    \n",
    "    cleantext = cleantext.replace('&nbsp;', ' ')\n",
    "    cleantext = cleantext.replace('\\n\\n', ' ')\n",
    "    cleantext = cleantext.replace('\\r\\n', ' ')\n",
    "    cleantext = cleantext.replace('\\n', ' ')    \n",
    "    cleantext = cleantext.strip()\n",
    "    \n",
    "    return cleantext\n",
    "\n",
    "def get_embed_content_entry(entry_data):\n",
    "    if entry_data['title'] or entry_data['caption']:\n",
    "        return {\n",
    "            'type': entry_data['acf_fc_layout'],\n",
    "            'title': entry_data['title'],\n",
    "            'text': entry_data['caption'],\n",
    "        }\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def get_text_content_entry(entry_data):\n",
    "    text = get_text_from_html(entry_data['text'])\n",
    "    \n",
    "    if text:\n",
    "        return {\n",
    "            'type': entry_data['acf_fc_layout'],\n",
    "            'text': text,\n",
    "        }\n",
    "\n",
    "    return {}\n",
    "\n",
    "def get_image_content_entry(entry_data):    \n",
    "    text_elements = []\n",
    "    for image_data in entry_data['image']:\n",
    "        if image_data['caption']:\n",
    "            text_elements.append(image_data['caption'].strip())\n",
    "        if image_data['description']:\n",
    "            text_elements.append(image_data['description'].strip())\n",
    "    \n",
    "    text = ' '.join(text_elements)\n",
    "    \n",
    "    if text:\n",
    "        return {\n",
    "            'type': entry_data['acf_fc_layout'],\n",
    "            'text': get_text_from_html(text)\n",
    "        }\n",
    "    \n",
    "    return {}\n",
    "    \n",
    "\n",
    "def get_essentialize_contribution_data(raw_data):\n",
    "    return get_essentialize_project_data(raw_data)\n",
    "\n",
    "def get_essentialize_field_note_data(raw_data):\n",
    "    data = {}\n",
    "    \n",
    "    data['link'] = 'https://notes.anthropocene-curriculum.org/id/%s' % raw_data['id']\n",
    "    \n",
    "    data['text'] = get_text_from_html(raw_data['acf']['text'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_essentialize_project_data(raw_data):\n",
    "    data = {}\n",
    "    \n",
    "    # title\n",
    "    data['title'] = raw_data['title']['rendered']\n",
    "    \n",
    "    # subtitle\n",
    "    data['subtitle'] = raw_data['acf']['subtitle']\n",
    "    \n",
    "    # abstract\n",
    "    data['abstract'] = get_text_from_html(raw_data['acf']['abstract'])\n",
    "    \n",
    "    # link\n",
    "    data['link'] = raw_data['link'].replace('content.', 'www.')\n",
    "        \n",
    "    # content\n",
    "    data['content'] = []\n",
    "    if raw_data['acf']['_content']['content']:\n",
    "        for entry in raw_data['acf']['_content']['content']:\n",
    "            \n",
    "            ignored_types = ['iframe', 'featured', 'featured-children', 'reading-list', 'pdf']\n",
    "        \n",
    "            # Get content based on layout type\n",
    "            content = {}\n",
    "            if entry['acf_fc_layout'] == 'embed':\n",
    "                content = get_embed_content_entry(entry)\n",
    "            elif entry['acf_fc_layout'] == 'text':\n",
    "                content = get_text_content_entry(entry)\n",
    "            elif entry['acf_fc_layout'] == 'image':\n",
    "                content = get_image_content_entry(entry)\n",
    "                \n",
    "            elif entry['acf_fc_layout'] in ignored_types:\n",
    "                # doesn't hold any content. skipped for now\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                print('Warning: Unhandled content type %s.' % entry['acf_fc_layout'])\n",
    "            \n",
    "            # Add content if not empty\n",
    "            if content:\n",
    "                data['content'].append(content)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Merge content files\n",
    "\n",
    "with open(content_list_filepath) as content_list_file:\n",
    "    data = json.load(content_list_file)\n",
    "    \n",
    "    for entry in data:\n",
    "        filepath = 'dmacp_data/%s/%s.json' % (entry['type'], entry['slug'])\n",
    "        \n",
    "        with open(filepath) as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "        \n",
    "            essentialized_data = {}\n",
    "            if entry['type'] == 'contribution':\n",
    "                essentialized_data = get_essentialize_contribution_data(json_data[0])\n",
    "            elif entry['type'] == 'field_note':\n",
    "                essentialized_data = get_essentialize_field_note_data(json_data[0])\n",
    "            elif entry['type'] == 'project':\n",
    "                essentialized_data = get_essentialize_project_data(json_data[0])\n",
    "        \n",
    "            merged_content_data[entry['type']]['data'][entry['slug']] = essentialized_data\n",
    "\n",
    "for category in merged_content_data:\n",
    "    filepath = merged_content_data[category]['filepath']\n",
    "    data = merged_content_data[category]['data']\n",
    "    \n",
    "    with open(filepath, 'w') as outfile:\n",
    "        json.dump(data, outfile, sort_keys=True, indent=4)\n",
    "\n",
    "num_contributions = len(merged_content_data['contribution']['data'])\n",
    "num_field_notes = len(merged_content_data['field_note']['data'])\n",
    "num_projects = len(merged_content_data['project']['data'])\n",
    "print('Merged %d contributions, %d field_notes, and %d projects.' % (num_contributions, num_field_notes, num_projects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUPDfLBBW_6r"
   },
   "source": [
    "## Run text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf40_waDW_Ej",
    "outputId": "e31d98e3-23f3-495e-8ce4-05e3442cf570"
   },
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# compressed content files\n",
    "contributions_file = data_path+'/contributions.json'\n",
    "field_notes_file = data_path+'/field_notes.json'\n",
    "projects_file = data_path+'/projects.json'\n",
    "\n",
    "# Some helper functions\n",
    "\n",
    "def get_merged_contribution_text(contribution_data):\n",
    "    texts = [contribution_data['abstract']]\n",
    "    for entry in contribution_data['content']:\n",
    "        texts.append(entry['text'])\n",
    "    return ' '.join(texts)\n",
    "\n",
    "def get_merged_field_note_text(field_note_data):\n",
    "    return field_note_data['text']\n",
    "\n",
    "def get_merged_project_text(project_data):\n",
    "    texts = [project_data['abstract']]\n",
    "    for entry in project_data['content']:\n",
    "        texts.append(entry['text'])\n",
    "    return ' '.join(texts)\n",
    "\n",
    "# --- Your code here: ---\n",
    "\n",
    "\n",
    "# Some field note example code …     \n",
    "with open(field_notes_file) as f:\n",
    "    field_notes = json.load(f)\n",
    "    \n",
    "    field_note = field_notes['1566946414604-johnwkim']\n",
    "    \n",
    "    print(field_note['link'])\n",
    "\n",
    "    text = get_merged_field_note_text(field_note)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Find named entities, phrases and concepts\n",
    "    for entity in doc.ents:\n",
    "        print(entity.text, entity.label_)\n",
    "        \n",
    "\n",
    "# Some project example code …\n",
    "with open(projects_file) as f:\n",
    "    projects = json.load(f)\n",
    "    \n",
    "    project = projects['essay-series']\n",
    "    \n",
    "    print(project['title'])\n",
    "    print(project['link'])\n",
    "\n",
    "    text = get_merged_project_text(project)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Find named entities, phrases and concepts\n",
    "    for entity in doc.ents:\n",
    "        print(entity.text, entity.label_)\n",
    "        \n",
    "\n",
    "# Some contribution example code …\n",
    "\n",
    "with open(contributions_file) as f:\n",
    "    contributions = json.load(f)\n",
    "    \n",
    "    contribution = contributions['navigating-the-anthropocene-river']\n",
    "    \n",
    "    print(contribution['title'])\n",
    "    print(contribution['link'])\n",
    "\n",
    "    text = get_merged_contribution_text(contribution)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Find named entities, phrases and concepts\n",
    "    for entity in doc.ents:\n",
    "        print(entity.text, entity.label_)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dmacp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
